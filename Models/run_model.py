import json
import logging
from datasets import metric
import nltk
import numpy as np
import pandas as pd
import tqdm as notebook_tqdm
from nltk.stem import SnowballStemmer
from rouge import Rouge
from simpletransformers.classification import (ClassificationArgs,
                                               ClassificationModel)

nltk.download("punkt")
import argparse
from ItaRouge import *
from AmRouge import *
from HmRouge import *


parser = argparse.ArgumentParser(description='Please select arithmetic_mean_bert or harmonic_mean_bert')
parser.add_argument('--model', type=str, default='harmonic_mean_bert', help='please select arithmetic_mean or harmonic_mean')
args = parser.parse_args()


def prepare_results(p, r, f):
    return "\t{}:\t{}: {:5.2f}\t{}: {:5.2f}\t{}: {:5.2f}".format(
        metric, "P", 100.0 * p, "R", 100.0 * r, "F1", 100.0 * f
    )


def print_scores(scores_dict):
    for k, v in scores_dict.items():
        print(f"{k}:")
        for metric, values in v.items():
            f1 = round(values['f'] * 100, 2)
            p = round(values['p'] * 100, 2)
            r = round(values['r'] * 100, 2)
            print(f"\t{metric}: P: {p}\tR: {r}\tF1: {f1}")

            
def prepare_results(p, r, f):
    return "\t{}:\t{}: {:5.2f}\t{}: {:5.2f}\t{}: {:5.2f}".format(
        metric, "P", 100.0 * p, "R", 100.0 * r, "F1", 100.0 * f
    )


def print_scores(scores_dict):
    for k, v in scores_dict.items():
        print(f"{k}:")
        for metric, values in v.items():
            f1 = round(values['f'] * 100, 2)
            p = round(values['p'] * 100, 2)
            r = round(values['r'] * 100, 2)
            print(f"\t{metric}: P: {p}\tR: {r}\tF1: {f1}")

            
def generate_summary_eval(row, model):
    '''
    generates bert summaries for k values of 3,5,7
    Args:
      row: row of the dataframe
      model: model
      Returns:
        bert_summary_3: bert summary for k=3
        bert_summary_5: bert summary for k=5
        bert_summary_7: bert summary for k=7
        '''
    doc = row['doc']
    summary = row['summary']
    sentences = doc.split('\n')
    predictions, raw_outputs = model.predict(sentences)
    most_import_sentence_indices = np.argsort(-predictions)
    bert_summary_3 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:3])])
    bert_summary_5 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:5])])
    bert_summary_7 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:7])])
    return bert_summary_3, bert_summary_5, bert_summary_7


def get_scores_eval(df, model):
    '''
    generates bert summaries for k values of 3,5,7 and calculates rouge scores
    Args:
      df: dataframe -> Validation data
      model: model
      Returns:
        scores_dict: dictionary of rouge scores
        '''
    scores_dict = {}
    evaluator = ItaRouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],
                          max_n=2,
                          apply_avg=True,
                          apply_best=False,
                          alpha=0.5, # Default F1_score
                          weight_factor=1.2,
                          stemming=True)
    df['bert_summary_3'], df['bert_summary_5'], df['bert_summary_7'] = zip(*df.apply(lambda row: generate_summary_eval(row, model), axis=1))
    all_hypothesis = df['bert_summary_3'].values
    all_references = df['summary'].values
    scores_dict[f'bert_summary_3'] = evaluator.get_scores(all_hypothesis, all_references)

    all_hypothesis = df['bert_summary_5'].values
    scores_dict[f'bert_summary_5'] = evaluator.get_scores(all_hypothesis, all_references)

    all_hypothesis = df['bert_summary_7'].values
    scores_dict[f'bert_summary_7'] = evaluator.get_scores(all_hypothesis, all_references)

    return scores_dict


def generate_test_summary(row, model):
    ''' 
    generate summary for each row in the test dataframe
    Args:
      row: row in the dataframe
      model: model (the one that we trained))
    Returns:
      bert_summary: summary generated by the model on k=5 sentences
    '''
    doc = row["doc"]
    summary = row["summary"]
    sentences = doc.split("\n")
    predictions, raw_outputs = model.predict(sentences)
    most_import_sentence_indices = np.argsort(-predictions)
    bert_summary = "\n".join(
        [sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:5])]
    )
    return bert_summary


def get_test_results(df):
    ''' 
    generate rouge scores for each row in the test dataframe
    Args:
      df: dataframe (test)
    returns: rouge scores
    '''
    evaluator = ItaRouge(
        metrics=["rouge-n", "rouge-l", "rouge-w"],
        max_n=2,
        apply_avg=True,
        apply_best=False,
        alpha=0.5,  # Default F1_score
        weight_factor=1.2,
        stemming=True,
    )
    all_hypothesis = df["bert_summary"].values
    all_references = df["summary"].values
    return evaluator.get_scores(all_hypothesis, all_references)


def training(df_train_sentences,df_val_sentences):
    '''
    generate rouge scores for each sentence in the train and validation dataframes
    model training
    evaluating the model for optimal k value
    testing the model
    results are exported to a json file to output folder
    '''
    train_df = df_train_sentences[["text", "label"]]
    eval_df = df_val_sentences[["text", "label"]]
    """ Training starts here"""
    logging.basicConfig(level=logging.DEBUG)
    transformers_logger = logging.getLogger("transformers")
    transformers_logger.setLevel(logging.INFO)
    max_seq_length = 256
    batch_size = 16
    # Enabling regression
    model_args = ClassificationArgs()
    model_args.num_train_epochs = 4
    model_args.evaluate_during_training = True
    model_args.overwrite_output_dir = True
    model_args.reprocess_input_data = True
    model_args.use_early_stopping = True
    model_args.early_stopping_metric = "eval_loss"
    model_args.early_stopping_patience = 15
    model_args.regression = True
    model_args.evaluate_during_training_steps = 1500
    model_args.max_seq_length = max_seq_length
    model_args.manual_seed = 42
    model_args.train_batch_size = batch_size
    model_args.eval_batch_size = batch_size
    model_args.best_model_dir = (
        "./outputs/best_model"
    ) #change this directory based on the model you run, ex: outputs/am_bert/best_model for am_bert
    model_args.save_eval_checkpoints = False
    model_args.save_model_every_epoch = False

    model = ClassificationModel(
        "bert",
        "dlicari/Italian-Legal-BERT",
        num_labels=1,
        args=model_args,
        use_cuda=True,
    )
    # Train model
    _, training_details = model.train_model(train_df, eval_df=eval_df)
    # show training details
    logging.info("training details")
    logging.info(pd.DataFrame(training_details))
    model = ClassificationModel(model.args.model_type, model.args.best_model_dir)
    return model


def run_am(train, test, val):
    ''' 
    trains and predicts arithmetic mean bert
    Args:
      train, test, val datasets
    Returns:
      saves the trained model in directory 'outputs/best_model'  
      Prints the results of validation k= 3,5,7 and test results
    '''
    df_train_sentences = generate_am_rouge_scores(train)
    df_val_sentences = generate_am_rouge_scores(val)
    model = training(df_train_sentences, df_val_sentences)
    scores_eval = get_scores_eval(val, model)
    print_scores(scores_eval)

    with open('./outputs/eval_am_bert_scores.json', 'w') as fp:
          json.dump(scores_eval, fp)
    test["bert_summary"] = test.apply(
    generate_test_summary, args=(model,), axis=1)
    scores_test_am = get_test_results(test)
    with open("./outputs/test_am_bert_scores.json", "w") as fp:
          json.dump(scores_test_am, fp)
    #remove this to not print results
    for metric, values in scores_test_am.items():
        precision = round(values['p'] * 100, 2)
        recall = round(values['r'] * 100, 2)
        f1_score = round(values['f'] * 100, 2)
        print(f"\t{metric}: P: {precision:>6.2f} R: {recall:>6.2f} F1: {f1_score:>6.2f}")


def run_hm(train, test, val):
    ''' 
    trains and predicts harmonic mean bert
    Args:
      train, test, val datasets
    Returns:
      saves the trained model in directory 'outputs/best_model'  
      Prints the results of validation k= 3,5,7 and test results
    '''
    df_train_sentences = generate_hm_rouge_scores(train)
    df_val_sentences = generate_hm_rouge_scores(val)
    model = training(df_train_sentences, df_val_sentences)
    scores_eval = get_scores_eval(val, model)
    print_scores(scores_eval)

    with open('./outputs/eval_hm_bert_scores.json', 'w') as fp:
          json.dump(scores_eval, fp)
    test["bert_summary"] = test.apply(
    generate_test_summary, args=(model,), axis=1)
    scores_test_hm = get_test_results(test)
    with open("./outputs/test_hm_bert_scores.json", "w") as fp:
          json.dump(scores_test_hm, fp)
    #remove this to not print results
    for metric, values in scores_test_hm.items():
        precision = round(values['p'] * 100, 2)
        recall = round(values['r'] * 100, 2)
        f1_score = round(values['f'] * 100, 2)
        print(f"\t{metric}: P: {precision:>6.2f} R: {recall:>6.2f} F1: {f1_score:>6.2f}")


def model_run():
    '''
    Args:
        model: either arithmetic_mean_bert or harmonic_mean_bert
    Returns:
        runs the model and prints the results
    '''
    train = pd.read_csv('./Datasets/df_train.csv')
    test  = pd.read_csv('./Datasets/df_test.csv')
    val  = pd.read_csv('./Datasets/df_val.csv')

    if args.model == 'arithmetic_mean_bert':
        run_am(train,test,val)
    elif args.model == 'harmonic_mean_bert':
        run_hm(train,test,val)
    else:
        parser.error(args,'please use either harmonic_mean_bert or arithmetic_mean_bert as input')
        

if __name__ == "__main__":
    model_run()