import json
import logging
from datasets import metric
import nltk
import numpy as np
import pandas as pd
import tqdm as notebook_tqdm
from nltk.stem import SnowballStemmer
from rouge import Rouge
from simpletransformers.classification import (ClassificationArgs,
                                               ClassificationModel)

nltk.download("punkt")
from pathlib import Path
import argparse, os
from ItaRouge import *
from AmRouge import *
from HmRouge import *


parser = argparse.ArgumentParser(description='Please select arithmetic_mean or harmonic_mean')
parser.add_argument('--model', type=str, default='harmonic_mean', help='please select arithmetic_mean or harmonic_mean')
parser.add_argument('--encoder', type=str, default = 'dlicari/Italian-Legal-BERT', help = 'please select only encoders mentioned in paper')
parser.add_argument('--path', type = str, default = './outputs/best_model')
args = parser.parse_args()

def dir_path(path):
    if os.path.isdir(path):
        return path
    else:
        os.path.mkdir(path)

def prepare_results(p, r, f):
    return "\t{}:\t{}: {:5.2f}\t{}: {:5.2f}\t{}: {:5.2f}".format(
        metric, "P", 100.0 * p, "R", 100.0 * r, "F1", 100.0 * f
    )

def print_scores(scores_dict):
    for k, v in scores_dict.items():
        print(f"{k}:")
        for metric, values in v.items():
            f1 = round(values['f'] * 100, 2)
            p = round(values['p'] * 100, 2)
            r = round(values['r'] * 100, 2)
            print(f"\t{metric}: P: {p}\tR: {r}\tF1: {f1}")


def generate_summary_eval(row, model):
    '''
    generates bert summaries for k values of 3,5,7
    Args:
      row: row of the dataframe
      model: model
      Returns:
        bert_summary_3: bert summary for k=3
        bert_summary_5: bert summary for k=5
        bert_summary_7: bert summary for k=7
        '''
    doc = row['doc']
    summary = row['summary']
    sentences = doc.split('\n')
    predictions, raw_outputs = model.predict(sentences)
    most_import_sentence_indices = np.argsort(-predictions)
    bert_summary_3 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:3])])
    bert_summary_5 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:5])])
    bert_summary_7 = '\n'.join([sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:7])])
    return bert_summary_3, bert_summary_5, bert_summary_7


def get_scores_eval(df, model):
    '''
    generates bert summaries for k values of 3,5,7 and calculates rouge scores
    Args:
      df: dataframe -> Validation data
      model: model
      Returns:
        scores_dict: dictionary of rouge scores
        '''
    scores_dict = {}
    evaluator = ItaRouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],
                          max_n=2,
                          apply_avg=True,
                          apply_best=False,
                          alpha=0.5, # Default F1_score
                          weight_factor=1.2,
                          stemming=True)
    df['bert_summary_3'], df['bert_summary_5'], df['bert_summary_7'] = zip(*df.apply(lambda row: generate_summary_eval(row, model), axis=1))
    all_hypothesis = df['bert_summary_3'].values
    all_references = df['summary'].values
    scores_dict[f'bert_summary_3'] = evaluator.get_scores(all_hypothesis, all_references)

    all_hypothesis = df['bert_summary_5'].values
    scores_dict[f'bert_summary_5'] = evaluator.get_scores(all_hypothesis, all_references)

    all_hypothesis = df['bert_summary_7'].values
    scores_dict[f'bert_summary_7'] = evaluator.get_scores(all_hypothesis, all_references)

    return scores_dict


def generate_test_summary(row, model):
    ''' 
    generate summary for each row in the test dataframe
    Args:
      row: row in the dataframe
      model: model (the one that we trained))
    Returns:
      bert_summary: summary generated by the model on k=5 sentences
    '''
    doc = row["doc"]
    summary = row["summary"]
    sentences = doc.split("\n")
    predictions, raw_outputs = model.predict(sentences)
    most_import_sentence_indices = np.argsort(-predictions)
    bert_summary = "\n".join(
        [sentences[idx].strip() for idx in np.sort(most_import_sentence_indices[0:5])]
    )
    return bert_summary


def get_test_results(df):
    ''' 
    generate rouge scores for each row in the test dataframe
    Args:
      df: dataframe (test)
    returns: rouge scores
    '''
    evaluator = ItaRouge(
        metrics=["rouge-n", "rouge-l", "rouge-w"],
        max_n=2,
        apply_avg=True,
        apply_best=False,
        alpha=0.5,  # Default F1_score
        weight_factor=1.2,
        stemming=True,
    )
    all_hypothesis = df["bert_summary"].values
    all_references = df["summary"].values
    return evaluator.get_scores(all_hypothesis, all_references)


def f_encoder(encoder):
    '''
    Converting huggingface model path to model name mentioned in paper.
    To store results based on model name from paper
    Args:
        Encoder: Huggingface model path
    returns:
        returns model name from paper
    '''
    f_encoder = ''
    if encoder == 'dlicari/Italian-Legal-BERT':
      f_encoder = 'italian_legal_bert'
    elif encoder == 'dbmdz/bert-base-italian-cased':
      f_encoder ='italian_bert'
    else:
      f_encoder = 'italian_legal_bert_sc'
    return f_encoder


def training(df_train_sentences,df_val_sentences,model_type, encoder):
    '''
    generate rouge scores for each sentence in the train and validation dataframes
    model training
    evaluating the model for optimal k value
    testing the model
    results are exported to a json file to output folder
    '''
    train_df = df_train_sentences[["text", "label"]]
    eval_df = df_val_sentences[["text", "label"]]
    """ Training starts here"""
    logging.basicConfig(level=logging.DEBUG)
    transformers_logger = logging.getLogger("transformers")
    transformers_logger.setLevel(logging.INFO)
    max_seq_length = 256
    batch_size = 16
    # Enabling regression
    model_args = ClassificationArgs()
    model_args.num_train_epochs = 4
    model_args.evaluate_during_training = True
    model_args.overwrite_output_dir = True
    model_args.reprocess_input_data = True
    model_args.use_early_stopping = True
    model_args.early_stopping_metric = "eval_loss"
    model_args.early_stopping_patience = 15
    model_args.regression = True
    model_args.evaluate_during_training_steps = 1500
    model_args.max_seq_length = max_seq_length
    model_args.manual_seed = 42
    model_args.train_batch_size = batch_size
    model_args.eval_batch_size = batch_size
    model_args.best_model_dir = (
        f".outputs/best_model"
    )
    model_args.save_eval_checkpoints = False
    model_args.save_model_every_epoch = False

    model = ClassificationModel(
        model_type = model_type,
        model_name = encoder,
        num_labels=1,
        args=model_args,
        use_cuda=True,
    )
    # Train model
    _, training_details = model.train_model(train_df, eval_df=eval_df)
    # show training details
    logging.info("training details")
    logging.info(pd.DataFrame(training_details))
    model = ClassificationModel(model.args.model_type, model.args.best_model_dir)
    return model



def run_am(train, test, val, output_path, model_type, encoder):
    '''
    Calculates the Arithmetic Mean BERT.
    Args: 
        train, test and validation dataset
        output_path: To store results
        model_type: bert, or camembert architecture
        encoder: the model path from huggingface
    Returns:
        returns results into the output path folder
    '''
    df_train_sentences = generate_am_rouge_scores(train)
    df_val_sentences = generate_am_rouge_scores(val)
    path_encoder = f_encoder(encoder)
    model = training(
        df_train_sentences, 
        df_val_sentences, 
        model_type = model_type,
        encoder = encoder
        )
    scores_eval = get_scores_eval(val, model)
    print_scores(scores_eval)   
    Path(f"{output_path}").mkdir(parents=True, exist_ok=True)
    with open(f'{output_path}/am_{path_encoder}_val_scores.json', 'w') as fp:
          json.dump(scores_eval, fp)
    test["bert_summary"] = test.apply(generate_test_summary, args=(model,), axis=1)
    scores_test_am = get_test_results(test)
    with open(f"{output_path}/am_{path_encoder}_test_scores.json", "w") as fp:
          json.dump(scores_test_am, fp)
    #remove this to not print results
    for metric, values in scores_test_am.items():
        precision = round(values['p'] * 100, 2)
        recall = round(values['r'] * 100, 2)
        f1_score = round(values['f'] * 100, 2)
        print(f"\t{metric}: P: {precision:>6.2f} R: {recall:>6.2f} F1: {f1_score:>6.2f}")


def run_hm(train, test, val, output_path, model_type, encoder):
    '''
    Calculates the Harmonic Mean BERT.
    Args: 
        train, test and validation dataset
        output_path: To store results
        model_type: bert, or camembert architecture
        encoder: the model path from huggingface
    Returns:
        returns results into the output path folder
    '''
    df_train_sentences = generate_hm_rouge_scores(train)
    df_val_sentences = generate_hm_rouge_scores(val)
    path_encoder = f_encoder(encoder)
    model = training(
        df_train_sentences, 
        df_val_sentences,
        model_type = model_type,
        encoder = encoder
        )
    scores_eval = get_scores_eval(val, model)
    print_scores(scores_eval)   
    Path(f"{output_path}").mkdir(parents=True, exist_ok=True)
    with open(f'{output_path}/hm_{path_encoder}_val_scores.json', 'w') as fp:
          json.dump(scores_eval, fp)
    test["bert_summary"] = test.apply(generate_test_summary, args=(model,), axis=1)
    scores_test_hm = get_test_results(test)
    with open(f"{output_path}/hm_{path_encoder}_test_scores.json", "w") as fp:
          json.dump(scores_test_hm, fp)
    #remove this to not print results
    for metric, values in scores_test_hm.items():
        precision = round(values['p'] * 100, 2)
        recall = round(values['r'] * 100, 2)
        f1_score = round(values['f'] * 100, 2)
        print(f"\t{metric}: P: {precision:>6.2f} R: {recall:>6.2f} F1: {f1_score:>6.2f}")


def model_run():
    '''
    To run the model
    '''
    train = pd.read_csv('.data/df_train.csv')
    test  = pd.read_csv('.data/df_test.csv')
    val   = pd.read_csv('.data/df_val.csv')
    encoder1 = ['dlicari/Italian-Legal-BERT','dbmdz/bert-base-italian-cased']
    models   = ['harmonic_mean', 'arithmetic_mean']
    encoder2 = ['dlicari/Italian-Legal-BERT-SC']

    if(args.model in models and args.encoder in encoder1):
        print('model and encoder entered correctly')
        if args.model == 'arithmetic_mean':
            run_am(
                train,
                test,
                val, 
                args.path,
                model_type = 'bert',
                encoder = args.encoder)
        elif args.model == 'harmonic_mean':
            run_hm(
                train,
                test,
                val, 
                args.path,
                model_type = 'bert',
                encoder = args.encoder
            )
    elif(args.model in models and args.encoder in encoder2):
        print('model and encoder entered correctly')
        if args.model == 'arithmetic_mean':
            run_am(
                train,
                test,
                val, 
                args.path,
                model_type = 'camembert',
                encoder = args.encoder)
        elif args.model == 'harmonic_mean':
            run_hm(
                train,
                test,
                val, 
                args.path,
                model_type = 'camembert',
                encoder = args.encoder
            )
    else:
      raise argparse.ArgumentTypeError(f'Given model({args.model}) or Given encoder({args.encoder}) is not valid. Please use valid encoder and model')
      
        

if __name__ == "__main__":
    model_run()
